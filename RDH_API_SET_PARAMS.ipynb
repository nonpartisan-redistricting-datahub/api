{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script can be used to extract data from the RDH API. Designated API users can set their parameters in the cell below. Username/email, password, a list of states, and additional filtering criteria (optional) are the input parameters. Once these are set, run the whole script (\"Cell\" -> \"Run All\" if using Jupyter Notebook) to retrieve the data. Aside from setting the parameters, you should not have to modify any of the code to retrieve data from the API, which will be extracted to the directory in which this script is housed on your system. Please note that there is additional information about datasets on our website: redistrictingdatahub.org. Should you be interested in this information, please navigate to your state(s) of interest and then desired dataset(s). If you have any questions or requests about/for the script, accessing RDH data, or becoming an API user, please contact info@redistrictingdatahub.org. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS TO SET\n",
    "\n",
    "#username or email associated with your RDH account\n",
    "username_or_email = \"\"\n",
    "\n",
    "#password for your RDH account\n",
    "password = \"\" \n",
    "\n",
    "#You can retrieve datasets by state by typing out the full state name or postal code abbreviation (e.g. \"Alabama\" or \"alabama\" or \"AL\" or \"al\").\n",
    "#If you would like data for multiple states, please separate by comma (e.g. \"Wisconsin, mn\").'\n",
    "#Because of the limits of WordPress API, it can only retrieve a list of datasets for one state at a time (since many states have nearly 1,000 datasets), so if you are requesting data from multiple states this step may take several minutes, please be patient.\n",
    "#You may re-run again for any additional desired states (the script will ask you if you would like to re-run and you do not need to restart the script.\n",
    "#list your states as string \"AL, minnesota, Kentucky\" or in a list [\"alabama\",\"MN\",\"kentucky\"]. In either the list or string, please separate using commas.\n",
    "states = \"\" \n",
    "\n",
    "#You can filter datasets in the state(s) you designated with the criteria listed below. All filter options are case insensitive.\n",
    "#You may search by year as YYYY for all years from 2010 to 2021.\n",
    "#You may search by dataset type with the following names: ACS5, CVAP, Projection, election results, voter file, incumbent, disag.\n",
    "#You may search by geogrpahy with the following: precinct, block, block group, census tract, vtd, county, state, aiannh, zctas, senate districts, legislative districts, congressional districts, house of represenative districts (or other district names for the SLDL or SLDU for a given state -- \"districts\" will retrieve all district boundaries).\n",
    "#'***Please note that if you would like to retrieve the official redistricting dataset for your state, please use \"official\" (no quotations) in your query. Not all states will produce an offical dataset.\n",
    "#You may search by file type as CSV or SHP.\n",
    "additional_filtering = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the four libraries needed to run the script. If you do not have these, you may need to install.\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "from getpass import getpass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is the baseurl used to retrieve the list of datasets on the website.\n",
    "baseurl = 'https://redistrictingdatahub.org/wp-json/download/list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function retrieves a list of all datasets on the RDH site. In order to run, you must be an API user and registered with the RDH site.\n",
    "Inputs: username (string), password (string)\n",
    "Optional Inputs: baseurl\"\"\"\n",
    "\n",
    "def get_list(username, password, states, baseurl=baseurl):\n",
    "    print('Retrieving list of datasets on RDH Website...')\n",
    "    if type(states)!=type([]):\n",
    "        states = [states]\n",
    "    dfs = []\n",
    "    for i in states:\n",
    "        params = {}\n",
    "        params['username'] = username\n",
    "        params['password'] = password\n",
    "        params['format'] = 'csv'\n",
    "        params['states'] = i\n",
    "        r = requests.get(baseurl, params=params)\n",
    "        data = r.content\n",
    "        try:\n",
    "            df = pd.read_csv(io.StringIO(data.decode('utf-8')))\n",
    "        except:\n",
    "            print('There was an error retrieving the list of datasets, please check that you have the correct password and username')\n",
    "            return \n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_string(string_list, row):\n",
    "    if len(string_list)==0:\n",
    "        return True\n",
    "    for i in string_list:\n",
    "        if i not in row:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_states(state_list, row):\n",
    "    check_state = []\n",
    "    if state_list == ['']:\n",
    "        return True\n",
    "    else:\n",
    "        for i in state_list:\n",
    "            if i == row:\n",
    "                check_state.append(True)\n",
    "                return True\n",
    "            else:\n",
    "                check_state.append(False)\n",
    "        if any('True') in check_state:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_fullname(state):\n",
    "    state = state.lower()\n",
    "    keys = ['al','ak','az','ar','ca','co','ct','de','fl',\n",
    "              'ga','hi','id','il','in','ia','ks','ky','la','me',\n",
    "              'md','ma','mi','mn','ms','mo','mt','ne',\n",
    "              'nv','nh','nj','nm','ny','nc','nd','oh',\n",
    "              'ok','or','pa','ri','sc','sd','tn','tx',\n",
    "              'ut','vt','va','wa','wv','wi','wy']\n",
    "    values = ['Alabama','Alaska','Arizona','Arkansas','California','Colorado','Connecticut','Delaware','Florida',\n",
    "            'Georgia','Hawaii','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana','Maine',\n",
    "            'Maryland','Massachusetts','Michigan','Minnesota','Mississippi','Missouri','Montana','Nebraska',\n",
    "            'Nevada','New Hampshire','New Jersey','New Mexico','New York','North Carolina','North Dakota','Ohio',\n",
    "            'Oklahoma','Oregon','Pennsylvania','Rhode Island','South Carolina','South Dakota','Tennessee','Texas',\n",
    "            'Utah','Vermont','Virginia','Washington','West Virginia','Wisconsin','Wyoming']\n",
    "    values = [i.lower() for i in values]\n",
    "    dictionary = dict(zip(keys,values))\n",
    "    for k, v in dictionary.items():\n",
    "        if k == state:\n",
    "            return v\n",
    "        else:\n",
    "            continue\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_state_name(list_of_states):\n",
    "    new_list = []\n",
    "    for i in list_of_states:\n",
    "        state = assign_fullname(i)\n",
    "        new_list.append(state)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''This function extracts the data that meets input specifications to the current working directory. In order to run, you must be an API user and registered with the RDH site.\n",
    "Inputs: username or email (string), password (string), states (string/list), additional_filtering (string)\n",
    "Output: N/A'''\n",
    "def get_data(username_or_email, password, states,additional_filtering):\n",
    "    df = get_list(username_or_email, password,states)\n",
    "    #read in the list of data\n",
    "    for i in df.columns:\n",
    "        if 'Filter by state found 0 states or unknown states' in i:\n",
    "            print('*You did not specify the necessary states parameter.*')\n",
    "            return\n",
    "    if df.shape[0]<10:\n",
    "        print('\\nYou either have an incorrect username/password or you are not a designated API user. To try again, please re-run.')\n",
    "        print('If you continue to have problems or would like to become an API user, please email info@redistrictingdatahub.org')\n",
    "        return\n",
    "    params = {\n",
    "    'username': username_or_email,\n",
    "    'password': password}\n",
    "    #subset the df by the additional string info\n",
    "    df['Title_Format'] = df.apply(lambda x: ' '.join([x['Title'],x['Format']]),axis=1)\n",
    "    additional_filtering = additional_filtering.split(',')\n",
    "    additional_filtering = [i.strip().lower() for i in additional_filtering]\n",
    "    df['Subset'] = df['Title_Format'].apply(lambda x: check_string(additional_filtering,x.lower()))\n",
    "    df = df[df['Subset']==True].copy()\n",
    "    #take all of the urls in the subset df and split them to just get the baseurl of the dataset (no params)\n",
    "    urls = list(df['URL'])\n",
    "    new_urls = []\n",
    "    id_dict = {}\n",
    "    for i in urls:\n",
    "        new = i.split('?')[0]\n",
    "        dataset_id = i.split('&datasetid=')[1]\n",
    "        id_dict.update({new:dataset_id})\n",
    "        new_urls.append(new)\n",
    "    titles = list(df['Title_Format'])\n",
    "    if len(titles) == 0:\n",
    "        print('\\nThere are no datasets that currently meet your criteria. Please re-run with different criteria to extract data.')\n",
    "    else:\n",
    "        titles = ', '.join(titles)\n",
    "        print('\\nThe datasets to be extracted are: ', titles)\n",
    "    ftype = list(df['Format'])\n",
    "    data = dict(zip(new_urls,ftype))\n",
    "    counter = 1\n",
    "    #iterate over all of the new urls and retrieve the data\n",
    "    for i in new_urls:\n",
    "        print('Retrieving', str(counter), 'of',str(len(new_urls)),'files')\n",
    "        #get the data from the url and the params listed above\n",
    "        params.update({'datasetid':id_dict.get(i)})\n",
    "        response = requests.get(i,params)\n",
    "        #get the file name of the dataset\n",
    "        file_name = i.split('%2F')[-1]\n",
    "        file_name = file_name.split('/')[-1]\n",
    "        file_name_no_zip = file_name.split('.')[0]\n",
    "        zipdot = '.'+file_name.split('.')[1]\n",
    "        #because we have multiple datasets with the same name (for CSV and SHP), but we may want SHP or CSV, we need to make them unique filenames\n",
    "        for k,v in data.items():\n",
    "            if k == i:\n",
    "                dtype = '_'+v.lower()\n",
    "            else:\n",
    "                continue\n",
    "        #new filename\n",
    "        if dtype in file_name_no_zip:\n",
    "            dtype = ''\n",
    "        file_name = file_name_no_zip+dtype+zipdot\n",
    "        print('Retrieving ', file_name)\n",
    "        #write the data\n",
    "        file = open(file_name, \"wb\")\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "        counter = counter+1\n",
    "    print('\\nDone extracting datasets to current working directory.')\n",
    "    print('Please re-run to extract additional data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_versions():\n",
    "    pd_check = str((pd.__version__))=='1.3.1'\n",
    "    req_check = str(requests.__version__) == '2.25.1'\n",
    "    np_check = str(np.__version__)=='1.20.3'\n",
    "    if pd_check == False:\n",
    "        print('WARNING: You do not have the correct version of pandas to run this script. You may need to install pandas version 1.3.1 for this script to work.')\n",
    "    if req_check == False:\n",
    "        print('WARNING: You do not have the correct version of requests to run this script. You may need to install requests version 2.25.1 for this script to work.')\n",
    "    if np_check == False:\n",
    "        print('WARNING: You do not have the correct version of numpy to run this script. You may need to install numpy version 1.20.3 for this script to work.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(username_or_email = username_or_email,password = password,states=states,additional_filtering=additional_filtering):\n",
    "    check_versions()\n",
    "    get_data(username_or_email, password, states,additional_filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
