{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the two libraries needed to run the script\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the baseurl used to retrieve the list of datasets on the website\n",
    "baseurl = 'https://redistrictingdatahub.org/wp-json/download/list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function retrieves a list of all datasets on the RDH site. In order to run, you must be an API user and registered with the RDH site.\n",
    "Inputs: username (string), password (string)\n",
    "Optional Inputs: form (string -- either 'csv' or 'json'), baseurl\"\"\"\n",
    "def get_list(username, password, form='csv', baseurl=baseurl):\n",
    "    print('Retrieving list of datasets on RDH Website...')\n",
    "    params = {}\n",
    "    params['username'] = username\n",
    "    params['password'] = password\n",
    "    params['format'] = form\n",
    "    r = requests.get(baseurl, params=params)\n",
    "    #Write the request to a file in the current workig directory\n",
    "    open('RDH_FileListing.'+form, 'wb').write(r.content)\n",
    "    print('List of datasets retrieved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function reads in the list of datasets as a pandas dataframe.\n",
    "Optional input: form (string -- either 'csv' or 'json')\"\"\"\n",
    "def read_list(form='csv'):\n",
    "    if form == 'csv':\n",
    "        df = pd.read_csv(r'./RDH_FileListing.csv')\n",
    "    else:\n",
    "        df = pd.read_json(r'./RDH_FileListing.json')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This function checks all strings in a list and returns either True or False based on if the the string is in that row\n",
    "Inputs: string list (list), and row\n",
    "Outputs: T/F\"\"\"\n",
    "def check_string(string_list, row):\n",
    "    check_list = []\n",
    "    for i in string_list:\n",
    "        if i in row:\n",
    "            #If the string from the list is in the row being searched, add True to the check_list\n",
    "            check_list.append(True)\n",
    "        else:\n",
    "            #if it isn't, add False\n",
    "            check_list.append(False)\n",
    "    #make the list a set to eliminate duplicate values\n",
    "    check_list = set(check_list)\n",
    "    if len(check_list) == 1:\n",
    "        if any('True') in check_list:\n",
    "            #if the lenght of the set is 1 (one value between T/F) AND that value is True, return True\n",
    "            return True\n",
    "        else:\n",
    "            #If there is any False value, return False\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    username = str(input('RDH Username: '))\n",
    "    password = str(input('RDH Password: '))\n",
    "\n",
    "    state = str(input('What state do you want data for? '))\n",
    "    state = state.capitalize()\n",
    "    string = str(input('Any other parameters? Please separate by comma (e.g. VEST, 2011, precinct, shp, csv). '))\n",
    "    string = [i.strip() for i in string.split(',')]\n",
    "    inputs = [username,password,state,string]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''This function extracts the data that meets input specifications to the current working directory. In order to run, you must be an API user and registered with the RDH site.\n",
    "Inputs: username (string), password (string), state_name (string), add_string (list of strings)\n",
    "Output: N/A'''\n",
    "def get_data():\n",
    "    #get list of datasets\n",
    "    inputs = get_inputs()\n",
    "    username = inputs[0]\n",
    "    password = inputs[1]\n",
    "    state_name = inputs[2]\n",
    "    add_string = inputs[3]\n",
    "    get_list(username,password)\n",
    "    #read in the list\n",
    "    df = read_list()\n",
    "    params = (\n",
    "    ('username', username),\n",
    "    ('password', password),\n",
    "    )\n",
    "    #subset the df by state name\n",
    "    df['Subset'] = df['State'].apply(lambda x: True if x==state_name else False)\n",
    "    df = df[df['Subset']==True]\n",
    "    #subset the df by the additional string info\n",
    "    df['Subset'] = df['Title'].apply(lambda x: check_string(add_string,x))\n",
    "    #df['Subset'] = df['Title'].apply(lambda x: True if add_string in str(x) else False)\n",
    "    df = df[df['Subset']==True]\n",
    "    #take all of the urls in the subset df and split them to just get the baseurl of the dataset (no params)\n",
    "    urls = list(df['Download URL'])\n",
    "    new_urls = []\n",
    "    for i in urls:\n",
    "        new = i.split('?')[0]\n",
    "        new_urls.append(new)\n",
    "    titles = list(df['Title'])\n",
    "    print('The datasets to be extracted are: ', titles)\n",
    "    cont = str(input('Would you like to extract these datasets to your current working directory? (Yes/No)'))\n",
    "    if cont == 'Yes':\n",
    "        \n",
    "        counter = 1\n",
    "        #iterate over all of the new urls and retrieve the data\n",
    "        for i in new_urls:\n",
    "            print('Retrieving', str(counter), 'of',str(len(new_urls)),'files')\n",
    "            #get the data from the url and the params listed above\n",
    "            response = requests.get(i,params)\n",
    "            #get the file name of the dataset\n",
    "            file_name = i.split('%2F')[-1]\n",
    "            file_name = file_name.split('/')[-1]\n",
    "            file_name_no_zip = file_name.split('.')[0]\n",
    "            zipdot = '.'+file_name.split('.')[1]\n",
    "            #because we have multiple datasets with the same name (for CSV and SHP), but we may want SHP or CSV, we need to make them unique filenames\n",
    "            if 'csv' in i:\n",
    "                dtype = '_csv'\n",
    "            else:\n",
    "                dtype = '_shp'\n",
    "            #new filename\n",
    "            file_name = file_name_no_zip+dtype+zipdot\n",
    "            print('Retrieving ', file_name)\n",
    "            #write the data\n",
    "            file = open(file_name, \"wb\")\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "            #print(response.url)\n",
    "            counter = counter+1\n",
    "        print('Done extracting datasets to current working directory.')\n",
    "        print('Please re-run to extract additional data.')\n",
    "    else:\n",
    "        print('Data is not extracted. Please re-run if you would like to extract data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
